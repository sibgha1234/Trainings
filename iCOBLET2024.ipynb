{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFXWK1Iy1JilG+3yqG5xBu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ash100/Trainings/blob/main/iCOBLET2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WELCOME to all the participant of **\"Colaboratory, a gateway for Bioinformatics and Innovation\" 2024**\n",
        "\n",
        "##This training Session is Designed by **Dr. Ashfaq Ahmad** for the Participants attending **iCOBLET2024** at **University of Doha for Science and Technology**, Qatar.\n",
        "\n",
        "##I will explain all the steps in Parallel during this training."
      ],
      "metadata": {
        "id": "7muqt3dwGGlL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OmhunJZf_2P6"
      },
      "outputs": [],
      "source": [
        "#@title Prerequistite Installation and Import\n",
        "! pip install chembl_webresource_client\n",
        "! pip install rdkit-pypi\n",
        "\n",
        "import math\n",
        "from pathlib import Path\n",
        "from zipfile import ZipFile\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rdkit.Chem import PandasTools\n",
        "from chembl_webresource_client.new_client import new_client\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title As an example I will use P2Y purinoceptor 1 protein (UniProt Accession P47900)\n",
        "target = new_client.target\n",
        "target_query = target.search('CHEMBL4315')\n",
        "targets = pd.DataFrame.from_dict(target_query)\n",
        "targets"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pK9uWvDVPg8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Lets retrieve Bioactivity Data\n",
        "selected_target = targets.target_chembl_id[0]\n",
        "selected_target"
      ],
      "metadata": {
        "id": "cRBBy1YpPuay",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activity = new_client.activity\n",
        "res = activity.filter(target_chembl_id=selected_target).filter(standard_type=\"IC50\")\n",
        "df = pd.DataFrame.from_dict(res)\n",
        "df"
      ],
      "metadata": {
        "id": "5lh5bwpnP5lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Lets Download this initial data\n",
        "df.to_csv('data_1.csv', index=False)"
      ],
      "metadata": {
        "id": "HAjIqgUyP7_E",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data contains many columns. We will be interested in three columns;\n",
        "\n",
        "i. Compound ID\n",
        "\n",
        "ii. Compound SIMILES (it is a linear representation of a chemical compound)\n",
        "\n",
        "iii. Activity value (Normally in IC50 unit)\n",
        "\n",
        "**Now we are going to remove all other columns - Also we will delete the data with missing values for the above three points.**"
      ],
      "metadata": {
        "id": "yFdyJvkHQWyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Treating Missing Data\n",
        "df2 = df[df.standard_value.notna()]\n",
        "df2 = df[df.canonical_smiles.notna()]\n",
        "\n",
        "len(df2.canonical_smiles.unique())\n",
        "df2_nr = df2.drop_duplicates(['canonical_smiles'])\n",
        "df2_nr"
      ],
      "metadata": {
        "id": "S6G9NPE0RDBC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Removing the extra columns\n",
        "selection = ['molecule_chembl_id','canonical_smiles','standard_value']\n",
        "df3 = df2_nr[selection]\n",
        "df3\n"
      ],
      "metadata": {
        "id": "OeT88Xy5Re3j",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Now we are going to save this data\n",
        "df3.to_csv('Data_2.csv', index=False)"
      ],
      "metadata": {
        "id": "VKXFh2kSRu_S",
        "cellView": "form"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compound Labelling**\n",
        "\n",
        "Now, we will label all the compounds in the dataset into three categories;\n",
        "\n",
        "i. IC50 value < 1000 --------> **Active**\n",
        "\n",
        "ii. IC50 value  > 1000 and < 10000 -----------------> **Inactive**\n",
        "\n",
        "iii. IC50 value > 1000 and < 10000 --------------> **Intermediate**\n",
        "\n"
      ],
      "metadata": {
        "id": "RHx62tThR9PE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Lets starts processing\n",
        "df4 = pd.read_csv('Data_2.csv')\n",
        "\n",
        "bioactivity_threshold = []\n",
        "for i in df4.standard_value:\n",
        "  if float(i) >= 10000:\n",
        "    bioactivity_threshold.append(\"inactive\")\n",
        "  elif float(i) <= 1000:\n",
        "    bioactivity_threshold.append(\"active\")\n",
        "  else:\n",
        "    bioactivity_threshold.append(\"intermediate\")\n",
        "\n",
        "    bioactivity_class = pd.Series(bioactivity_threshold, name='class')\n",
        "df5 = pd.concat([df4, bioactivity_class], axis=1)\n",
        "\n",
        "\n",
        "df5"
      ],
      "metadata": {
        "id": "xOZtuPtJS6WO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5.to_csv('Data_3.csv', index=False)"
      ],
      "metadata": {
        "id": "QXU7v8e_Tatz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def pIC50(input):\n",
        "    pIC50 = []\n",
        "\n",
        "    for i in input['standard_value_norm']:\n",
        "        molar = i*(10**-9) # Converts nM to M\n",
        "        pIC50.append(-np.log10(molar))\n",
        "\n",
        "    input['pIC50'] = pIC50\n",
        "    x = input.drop('standard_value_norm', 1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "pJslDho5Uli9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5.standard_value.describe()"
      ],
      "metadata": {
        "id": "jjL1Y4iiUs6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-np.log10( (10**-9)* 100000000 )\n",
        "-np.log10( (10**-9)* 10000000000 )\n",
        "def norm_value(input):\n",
        "    norm = []\n",
        "\n",
        "    for i in input['standard_value']:\n",
        "        if i > 100000000:\n",
        "          i = 100000000\n",
        "        norm.append(i)\n",
        "\n",
        "    input['standard_value_norm'] = norm\n",
        "    x = input.drop('standard_value', 1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "M2rMfmuvU2ey"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Normalization function\n",
        "def norm_value(input_df):\n",
        "    # Normalize the 'standard_value' column\n",
        "    norm = (input_df['standard_value'] - input_df['standard_value'].min()) / (input_df['standard_value'].max() - input_df['standard_value'].min())\n",
        "    input_df['standard_value_norm'] = norm\n",
        "    x = input_df.drop('standard_value', axis=1)\n",
        "    return x\n",
        "\n",
        "# Assuming df_combined is already loaded\n",
        "df_norm = norm_value(df5)\n",
        "print(df_norm)"
      ],
      "metadata": {
        "id": "QbRnJGpkVANF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_norm"
      ],
      "metadata": {
        "id": "rYOWsLuPVDaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to calculate pIC50 values\n",
        "def pIC50(input_df):\n",
        "    # Assuming we are calculating pIC50 using the normalized values\n",
        "    input_df['pIC50'] = -np.log10(input_df['standard_value_norm'] * 1e-9)  # Example transformation\n",
        "    return input_df\n",
        "\n",
        "# Assuming df_norm is already created\n",
        "df_final = pIC50(df_norm)\n",
        "print(df_final)"
      ],
      "metadata": {
        "id": "ZQhqzGWfVkcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final"
      ],
      "metadata": {
        "id": "1WcXKVCiVpmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.to_csv('df_final.csv', index=False)"
      ],
      "metadata": {
        "id": "yFr3avrZ_IoL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Distribution plot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set figure size and DPI to control resolution\n",
        "width_pixels = 600\n",
        "dpi = 100  # For 600 pixels width with a 6-inch figure\n",
        "width_in_inches = width_pixels / dpi\n",
        "\n",
        "# Plot Bioactivity class frequency\n",
        "plt.figure(figsize=(width_in_inches, width_in_inches), dpi=dpi)\n",
        "\n",
        "sns.countplot(x='class', data=df_final, edgecolor='black')\n",
        "\n",
        "plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Frequency', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Save the plot as a PNG file\n",
        "plt.savefig(\"bioactivity_class_frequency.png\", format=\"png\", dpi=dpi)\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "scvHPPqOHe_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.to_csv('Data_Final.csv')"
      ],
      "metadata": {
        "id": "dBMKH2u4WBc6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Frequency plot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set up the figure with specified size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the histogram\n",
        "sns.histplot(df_final['pIC50'], kde=True)\n",
        "plt.title(\"Distribution of pIC50 Values\")\n",
        "plt.xlabel(\"pIC50\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Save the plot as a PNG file\n",
        "plt.savefig(\"pIC50_distribution.png\", format=\"png\", dpi=100)  # Adjust DPI for resolution as needed\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "CbiDYofOH4xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PIC50 vs Classes\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set up the figure with specified size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the boxplot\n",
        "sns.boxplot(data=df_final, x='class', y='pIC50')\n",
        "plt.title(\"pIC50 Across Bioassay Classes\")\n",
        "plt.xlabel(\"Bioassay Class\")\n",
        "plt.ylabel(\"pIC50\")\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Save the plot as a PNG file\n",
        "plt.savefig(\"pIC50_bioassay_classes.png\", format=\"png\", dpi=100)  # Adjust DPI for resolution as needed\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "sch_KkPSGQrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Compound activity measures**\n",
        "\n",
        "- **IC50** is the half maximal inhibitory concentration of a drug which indicates how much of a drug is needed to inhibit a biological process by half.\n",
        "- **pIC50** is the negative logarithm of the IC50 value. It is more easily interpretable than IC50 values and a common measure for potency of compounds."
      ],
      "metadata": {
        "id": "Sn9Byr4dXI3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Molecule encoding**\n",
        "\n",
        "For machine learning and Artificial Neural Networks algorithms, molecules need to be converted into a machine readable format, e.g. a list of features.\n",
        "\n",
        "Molecular fingerprints encode chemical structures and molecular features in a bit string, where at each position \"1\" represents the presence and \"0\" represents the absence of a feature. One of the common fingerprints used are **M**olecular **ACC**ess **S**ystem fingerprints [(MACCS Keys)](https://docs.eyesopen.com/toolkits/python/graphsimtk/fingerprint.html#maccs) which are 166 bits structural key descriptors in which each bit is associated with a [SMARTS](https://docs.eyesopen.com/toolkits/python/oechemtk/glossary.html#term-smarts) pattern encoding a specific substructure."
      ],
      "metadata": {
        "id": "xBde0sVPXdj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **What is a neural network?**\n",
        "\n",
        "Neural networks, also known as artificial neural networks (ANNs), are a subset of machine learning algorithms. The structure and the name of the neural network is inspired by the human brain, mimicking the way that biological neurons transfer signals to one another.\n",
        "\n",
        "![Basic structure](https://github.com/volkamerlab/teachopencadd/blob/master/teachopencadd/talktorials/T022_ligand_based_screening_neural_network/images/basic_structure.png?raw=1)\n",
        "\n",
        "*Figure 1:* The figure shows the basic structure of an artificial neural network. It is taken from the blogpost: \"*Designing Your Neural Networks*\", Lavanya Shukla, [towardsdatascience](https://towardsdatascience.com/designing-your-neural-networks-a5e4617027ed)."
      ],
      "metadata": {
        "id": "siwnh9voX82z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANNs consist of three main layers as shown in the figure above: the _input layer_, some _hidden layers_ and the _output layer_. Let's take a deeper look at each of them.\n",
        "\n",
        "1. **Input neurons or input layer**\n",
        "   - This layer represents the number of features which are used to make the predictions.\n",
        "   - The input vector needs one input neuron per feature.\n",
        "2. **Hidden layers and neurons per hidden layer**\n",
        "    - The dimension of the hidden layers may vary greatly, but a good rule of thumb is to have dimensions in the range of the input layer and the output layer.\n",
        "    - In general, using the same number of neurons for all hidden layers will suffice but for some datasets, having a large first layer and following it up with smaller layers may lead to a better performance as first layers can learn many low-level features.\n",
        "3. **Output neurons or output layer**\n",
        "    - The output layer represents the value of interest, which will be predicted by the neural network.\n",
        "        - Regression task: the value is a real number (or vector) such as the pIC50 value.\n",
        "        - Binary classification task: the output neuron represents the probability of belonging to the positive class.\n",
        "        - Multi-class classification task: there is one output neuron per class and the predictions represent the probability of belonging to each class. A certain activation function is applied on the output layer to ensure the final probabilities sum up to 1."
      ],
      "metadata": {
        "id": "scY5rsCUYUMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neurons** are the core units of a neural network. Let's look into the operations done by each neuron to understand the overall mechanism of a neural network.\n",
        "\n",
        "![Neuron](https://github.com/volkamerlab/teachopencadd/blob/master/teachopencadd/talktorials/T022_ligand_based_screening_neural_network/images/neuron.png?raw=1)\n",
        "\n",
        "*Figure 2:* Operations done by a neuron. The figure is taken from the blogpost: \"*First neural network for beginners explained (with code)*\", Arthur Arnx, [towardsdatascience](https://towardsdatascience.com/first-neural-network-for-beginners-explained-with-code-4cfd37e06eaf)."
      ],
      "metadata": {
        "id": "nh3GauKLYg1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keras workflow\n",
        "\n",
        "[Keras](https://keras.io/getting_started/) is an open-source library for machine learning and more specifically neural networks. Its API runs on top of the very well-known [tensorflow](https://www.tensorflow.org/) deep learning platform.\n",
        "\n",
        "Below, we present a common workflow for training a neural network with [keras](https://keras.io/getting_started/).\n",
        "\n",
        "\n",
        "- **Prepare the data** − Foremost for any machine learning algorithm, we process, filter and select only the required information from the data. Then, the data is split into training and test data sets. The test data is used to evaluate the prediction of the algorithm and to cross check the efficiency of the learning process.\n",
        "\n",
        "\n",
        "- **Define the model** - In keras, every ANN is represented by keras [models](https://keras.io/api/models/model/#model-class). Keras provides a way to create a model which is called [sequential](https://keras.io/api/models/sequential/). The layers are arranged sequentially where the data flows from one layer to another layer in a given order until the data finally reaches the output layer. Each layer in the ANN can be represented by a *keras layer*.\n",
        "\n",
        "\n",
        "- **Compile the model** − The compilation is the final step in creating a model. Once the compilation is done, we can move on to the training phase. A _loss function_ and an _optimizer_ are required in the learning phase to define the prediction error and to minimize it, respectively. In the practical part of this talktorial, we use the mean squared error as a loss and the [adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) optimizer, which is a popular version of gradient descent and has shown to give good results in a wide range of problems.\n",
        "\n",
        "- **Fit the model** - The actual learning process will be done in this phase using the training data set. We can call the [fit()](https://keras.io/api/models/model_training_apis/#fit-method) method which needs several parameters such as $x$ the input data, $y$ the target data, the batch size, the number of epochs, etc. An _epoch_ is when the entire dataset is passed forward and backward through the neural network once.\n",
        "\n",
        "- **Evaluate model** − We can evaluate the model by looking at the loss function between the predicted and true values of the test data using the [evaluate()](https://keras.io/api/models/model_training_apis/#evaluate-method) method.\n",
        "\n",
        "    - Scatter plots are a common and simple approach to visualize the evaluation of a model. They plot the predicted vs. true values. If the fit was perfect, we should see the $y=x$ line, meaning that the predicted value is exactly the true value.\n",
        "    \n",
        "    \n",
        "- **Predictions on external/unlabeled data** − We make predictions based on the trained model for the external data set using the [predict()](https://keras.io/api/models/model_training_apis/#predict-method) method."
      ],
      "metadata": {
        "id": "OOOyRG_jYyLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installation of some required packages\n",
        "! pip install seaborn\n",
        "! pip install matplotlib\n",
        "! pip install --upgrade keras\n",
        "! pip install --upgrade scikit_learn"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iflySVXyY9k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import the required libraries\n",
        "from pathlib import Path\n",
        "from warnings import filterwarnings\n",
        "\n",
        "# Silence some expected warnings\n",
        "filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import MACCSkeys, Draw, rdFingerprintGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "\n",
        "# Neural network specific libraries\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jmchw947ZP6a"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our data should contains three columns.\n",
        "\n",
        "i. Molecular ID\n",
        "\n",
        "ii. SMILES\n",
        "\n",
        "iii. pIC50\n",
        "\n",
        "\n",
        " How many columns it contains? Lets see"
      ],
      "metadata": {
        "id": "nTm4aG7CZj4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are going to upload our data after some manual cleaning"
      ],
      "metadata": {
        "id": "hVwBFhzvjH56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df = pd.read_csv(\"/content/Data_Final_2.csv\")\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "elYFhHhga1yL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at head\n",
        "df.head()"
      ],
      "metadata": {
        "id": "AClcmao_bIcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep necessary columns\n",
        "chembl_df = df[[\"canonical_smiles\", \"pIC50\"]]\n",
        "chembl_df.head()\n",
        "# NBVAL_CHECK_OUTPUT"
      ],
      "metadata": {
        "id": "b21BY6PPbRIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smiles_to_fp(smiles, method=\"maccs\", n_bits=2048):\n",
        "    \"\"\"\n",
        "    Encode a molecule from a SMILES string into a fingerprint.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    smiles : str\n",
        "        The SMILES string defining the molecule.\n",
        "\n",
        "    method : str\n",
        "        The type of fingerprint to use. Default is MACCS keys.\n",
        "\n",
        "    n_bits : int\n",
        "        The length of the fingerprint.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    array\n",
        "        The fingerprint array.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert smiles to RDKit mol object\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    if method == \"maccs\":\n",
        "        return np.array(MACCSkeys.GenMACCSKeys(mol))\n",
        "    if method == \"morgan2\":\n",
        "        fpg = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=n_bits)\n",
        "        return np.array(fpg.GetCountFingerprint(mol))\n",
        "    if method == \"morgan3\":\n",
        "        fpg = rdFingerprintGenerator.GetMorganGenerator(radius=3, fpSize=n_bits)\n",
        "        return np.array(fpg.GetCountFingerprint(mol))\n",
        "    else:\n",
        "        print(f\"Warning: Wrong method specified: {method}.\" \" Default will be used instead.\")\n",
        "        return np.array(MACCSkeys.GenMACCSKeys(mol))"
      ],
      "metadata": {
        "id": "hZvGYviqbZr0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chembl_df[\"fingerprints_df\"] = chembl_df[\"canonical_smiles\"].apply(smiles_to_fp)\n",
        "\n",
        "# Look at head\n",
        "print(\"Shape of dataframe:\", chembl_df.shape)\n",
        "chembl_df.head(3)\n",
        "# NBVAL_CHECK_OUTPUT"
      ],
      "metadata": {
        "id": "oXHicylpbeTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Split the data into training and test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    chembl_df[\"fingerprints_df\"], chembl_df[[\"pIC50\"]], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Print the shape of training and testing data\n",
        "print(\"Shape of training data:\", x_train.shape)\n",
        "print(\"Shape of test data:\", x_test.shape)\n",
        "# NBVAL_CHECK_OUTPUT"
      ],
      "metadata": {
        "id": "ljqaGOzcbtlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step-1: Define and Compile Neural Network Model\n",
        "def neural_network_model(hidden1, hidden2):\n",
        "    \"\"\"\n",
        "    Creating a neural network from two hidden layers\n",
        "    using ReLU as activation function in the two hidden layers\n",
        "    and a linear activation in the output layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    hidden1 : int\n",
        "        Number of neurons in first hidden layer.\n",
        "\n",
        "    hidden2: int\n",
        "        Number of neurons in second hidden layer.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model\n",
        "        Fully connected neural network model with two hidden layers.\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    # First hidden layer\n",
        "    model.add(Dense(hidden1, activation=\"relu\", name=\"layer1\"))\n",
        "    # Second hidden layer\n",
        "    model.add(Dense(hidden2, activation=\"relu\", name=\"layer2\"))\n",
        "    # Output layer\n",
        "    model.add(Dense(1, activation=\"linear\", name=\"layer3\"))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\", \"mae\"])\n",
        "    return model"
      ],
      "metadata": {
        "cellView": "form",
        "id": "i7qK5_hNbxbt"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural network parameters\n",
        "batch_sizes = [16, 32, 64]\n",
        "nb_epoch = 50\n",
        "layer1_size = 64\n",
        "layer2_size = 32"
      ],
      "metadata": {
        "id": "bam_joInb04D"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "sns.set(color_codes=True)\n",
        "for index, batch in enumerate(batch_sizes):\n",
        "    fig.add_subplot(1, len(batch_sizes), index + 1)\n",
        "    model = neural_network_model(layer1_size, layer2_size)\n",
        "\n",
        "    # Fit model on x_train, y_train data\n",
        "    history = model.fit(\n",
        "        np.array(list((x_train))).astype(float),\n",
        "        y_train.values,\n",
        "        batch_size=batch,\n",
        "        validation_data=(np.array(list((x_test))).astype(float), y_test.values),\n",
        "        verbose=0,\n",
        "        epochs=nb_epoch,\n",
        "    )\n",
        "    plt.plot(history.history[\"loss\"], label=\"train\")\n",
        "    plt.plot(history.history[\"val_loss\"], label=\"test\")\n",
        "    plt.legend([\"train\", \"test\"], loc=\"upper right\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylim((0, 15))\n",
        "    plt.title(\n",
        "        f\"test loss = {history.history['val_loss'][nb_epoch-1]:.2f}, \" f\"batch size = {batch}\"\n",
        "    )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0vOuv0SLb3VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "# Save the trained model\n",
        "filepath = \"/content/sample_data/best_weights.weights.h5\"\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath,\n",
        "    monitor=\"loss\",\n",
        "    verbose=0,\n",
        "    save_best_only=True,\n",
        "    mode=\"min\",\n",
        "    save_weights_only=True,\n",
        ")\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Fit the model\n",
        "model.fit(\n",
        "    np.array(list((x_train))).astype(float),\n",
        "    y_train.values,\n",
        "    epochs=nb_epoch,\n",
        "    batch_size=16,\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=0,\n",
        ")"
      ],
      "metadata": {
        "id": "tu0DLAFlcInl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model Evaluation\n",
        "print(f\"Evaluate the model on the test data\")\n",
        "scores = model.evaluate(np.array(list((x_test))), y_test.values, verbose=0)\n",
        "print(f\" loss: {scores[0]:.2f}\")\n",
        "print(f\" mse (same as loss): {scores[1]:.2f}\")\n",
        "print(f\" mae: {scores[2]:.2f}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "t1DACvkMcPG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title training and Prediction of pIC50 values on x_test data\n",
        "y_pred = model.predict(np.array(list((x_test))))\n",
        "\n",
        "# Print 5 first pIC50 predicted values\n",
        "first_5_prediction = [print(f\"{value[0]:.2f}\") for value in y_pred[0:5]]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SEQq6_vhcWdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot\n",
        "limits = 0, 15\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y_pred, y_test, marker=\".\")\n",
        "lin = np.linspace(*limits, 100)\n",
        "ax.plot(lin, lin)\n",
        "ax.set_aspect(\"equal\", adjustable=\"box\")\n",
        "ax.set_xlabel(\"Predicted values\")\n",
        "ax.set_ylabel(\"True values\")\n",
        "ax.set_title(\"Scatter plot: pIC50 values\")\n",
        "ax.set_xlim(limits)\n",
        "ax.set_ylim(limits)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e5aU-2G0cebD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_json\n",
        "\n",
        "# Serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Serialize weights to HDF5\n",
        "model.save_weights(\"model.weights.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "metadata": {
        "id": "K4cme_LLcjXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload Unknown Dataset\n",
        "external_data = pd.read_csv(\"/content/unk2.csv\", index_col=0)\n",
        "external_data = external_data.reset_index(drop=False)\n",
        "external_data.head()\n",
        "# NBVAL_CHECK_OUTPUT"
      ],
      "metadata": {
        "id": "jIUrt45LdaVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert SMILES strings to MACCS fingerprints\n",
        "external_data[\"fingerprints_df\"] = external_data[\"canonical_smiles\"].apply(smiles_to_fp)\n",
        "\n",
        "# Look at head\n",
        "print(\"Shape of dataframe : \", external_data.shape)\n",
        "external_data.head(3)\n",
        "# NBVAL_CHECK_OUTPUT"
      ],
      "metadata": {
        "id": "qt6jxarAdqgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load json and create model\n",
        "json_file = open(\"model.json\", \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model.weights.h5\")\n",
        "print(\"Loaded model from disk\")"
      ],
      "metadata": {
        "id": "dIBpWpGKeSqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Perform Predictions on Unknown Data\n",
        "predictions = model.predict(\n",
        "    np.array(list((external_data[\"fingerprints_df\"]))).astype(float), callbacks=callbacks_list\n",
        ")\n",
        "\n",
        "predicted_pIC50 = pd.DataFrame(predictions, columns=[\"predicted_pIC50\"])\n",
        "predicted_pIC50_df = external_data.join(predicted_pIC50)\n",
        "\n",
        "predicted_pIC50_df.head(4)"
      ],
      "metadata": {
        "id": "KHn5oNzaexcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the predicted values in a csv file in the data folder\n",
        "predicted_pIC50_df.to_csv(\"/content/predictions.csv\")"
      ],
      "metadata": {
        "id": "C3re_7BUfbY8"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CONGRATULATIONS FOR COMPLETING THIS SHORT TUTORIAL**\n",
        "\n",
        "\n",
        "#In Case You Want To Learn With Me, I Drop Interesting Tutorials on\n",
        "\n",
        "#**BIOINFORMATICS INSIGHTS** A YOUTUBE CHANNEL"
      ],
      "metadata": {
        "id": "YPHmbtsdhdRr"
      }
    }
  ]
}